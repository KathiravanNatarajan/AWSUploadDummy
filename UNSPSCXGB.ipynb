{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaterialDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRTEL BILLS 22aug TO 23 AUG    2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIRTEL MOBILE BILLS 23nov O 22 dec 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aluminum fabrication work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aluminum fabrication work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>civil &amp; plumbing work @ BMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MaterialDescription\n",
       "0    AIRTEL BILLS 22aug TO 23 AUG    2012\n",
       "1   AIRTEL MOBILE BILLS 23nov O 22 dec 12\n",
       "2               aluminum fabrication work\n",
       "3               aluminum fabrication work\n",
       "4             civil & plumbing work @ BMT"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('UNSPSCdataset.csv',encoding='mac_roman',low_memory=False)\n",
    "output = 'UNSPSC_Final'\n",
    "y = df[output]\n",
    "features = ['MaterialDescription']\n",
    "X = df[features]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Downloading stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n",
      "    Downloading package stopwords to /home/carnd/nltk_data...\n",
      "      Unzipping corpora/stopwords.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()  # Download text data sets, including stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set UNSPSC description...\n",
      "\n",
      "Description 20000 of 45001\n",
      "\n",
      "Description 40000 of 45001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "def description_to_words(review_text):\n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n",
    "# Get the number of reviews based on the dataframe column size\n",
    "num_description = df['MaterialDescription'].size\n",
    "print(\"Cleaning and parsing the training set UNSPSC description...\\n\")\n",
    "clean_description = []\n",
    "for i in range(0, num_description):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%20000 == 0 ):\n",
    "        print(\"Description %d of %d\\n\" % ( i+1, num_description))                                                                  \n",
    "    clean_description.append( description_to_words(df['MaterialDescription'][i] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating bag of words from the useful word extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "(45001, 10338)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 15000) \n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_description)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### XGBOOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame(train_data_features)\n",
    "print(type(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\gpu\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda\\envs\\gpu\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ny = df[output]\\nfeatures = ['MaterialDescription']\\nX = df[features]\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\"\"\"\n",
    "y = df[output]\n",
    "features = ['MaterialDescription']\n",
    "X = df[features]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, new_df, y,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(new_df.values, label=y.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(new_df, y,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(new_df)\n",
    "    dtrain_predprob = alg.predict_proba(new_df)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n",
    "    \n",
    "    \"\"\"\n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective='multi:softmax',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, new_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Touch below after fixing above issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49657, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "desc 20000 of 49657\n",
      "\n",
      "desc 40000 of 49657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = pd.read_csv('UNSPSCtestDataSet.csv',encoding='mac_roman',low_memory=False)\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print(test.shape)\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_desc = len(test[\"MaterialDescription\"])\n",
    "clean_test_desc = [] \n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,num_desc):\n",
    "    if( (i+1) % 20000 == 0 ):\n",
    "        print(\"desc %d of %d\\n\" % (i+1, num_desc))\n",
    "    clean_desc = description_to_words(test[\"MaterialDescription\"][i])\n",
    "    clean_test_desc.append(clean_desc)\n",
    "    \n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_desc)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"unspscRFcode\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"finalunspscRF8000-500.csv\", index=False)\n",
    "\n",
    "import numpy as np\n",
    "fromcsv = pd.read_csv(\"finalunspscRF8000-500.csv\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test['UNSPSC_Final'],fromcsv['unspscRFcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
